import OpenAI from "openai";
import { Langfuse } from "langfuse";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";

// ä¸»å‹•åˆå§‹åŒ– Langfuse
const langfuse = new Langfuse({
  publicKey: process.env.LANGFUSE_PUBLIC_KEY,
  secretKey: process.env.LANGFUSE_SECRET_KEY,
  baseUrl: process.env.LANGFUSE_HOST,
});

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const GPT_MODEL = "gpt-4.1-mini";

// å®šç¾©å›ç­” schema
const TurtleSoupAnswer = z.object({
  answer: z.enum(["æ˜¯", "å¦", "ä¸ç›¸é—œ"]),
  clue: z.string(),
  canReveal: z.boolean(),
});

export async function askLLM({
  puzzle,
  answer,
  question,
  clues = [],
}: {
  puzzle: string;
  answer: string;
  question: string;
  clues?: string[];
}) {
  // å–å¾— production prompt å…§å®¹
  const prompt = await langfuse.getPrompt("turtlesoup-qa");
  const compiledPrompt = prompt.compile({
    puzzle,
    answer,
    question,
    clues: clues.length > 0 ? clues.join("ï¼›") : "ï¼ˆç„¡ï¼‰",
  });
  console.log(
    "%c ğŸª: compiledPrompt ",
    "font-size:16px;background-color:#2588d5;color:white;",
    compiledPrompt
  );

  // Debug: ç¢ºèªæœ‰å‘¼å«åˆ° openai
  console.log("Calling openai.responses.parse...");

  const response = await openai.responses.parse({
    model: GPT_MODEL,
    input: compiledPrompt,
    text: {
      format: zodTextFormat(TurtleSoupAnswer, "answer"),
    },
    // max_tokens: 200,
    temperature: 0,
  });

  const result = response.output_parsed;

  // Langfuse è¿½è¹¤ï¼šæ¯æ¬¡å•ç­”éƒ½å»ºç«‹ä¸€å€‹ trace + generation
  const trace = langfuse.trace({
    name: "TurtleSoup LLM QA",
    metadata: { puzzle, question, clues },
    input: compiledPrompt,
    output: result,
  });
  trace
    .generation({
      name: "llm-ask",
      model: GPT_MODEL,
      input: prompt,
      output: result?.answer,
      metadata: result,
    })
    .end();
  await langfuse.shutdownAsync(); // ç¢ºä¿ event é€å‡º

  return result;
}
